{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 :\n",
    "\n",
    "#### Definition of the MLP class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# MLP class :\n",
    "\n",
    "# The model implemented as follows :\n",
    "# Each layers is represented by a b vector (biases) and a W matrix (weights)\n",
    "# These are referenced by the weights dictionary. The format is :\n",
    "# self.weights[f\"X{n}\"] where X = b, W\n",
    "# NB : In our implementation, these matrices are transposed compared to the class notations\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_dims=(1024, 2048), # dimensions of each hidden layers\n",
    "                 n_hidden=2, # number of hidden layers\n",
    "                 mode='train', # current mode : train/test\n",
    "                 datapath=None, # path where to find the .pkl file\n",
    "                 model_path=None, # path where to save/load the model \n",
    "                 epsilon = 1e-6, # for cross entropy calculus stability : log(x) = log(epsilon) if x < epsilon\n",
    "                 lr = 1e-1, # learning rate\n",
    "                 n_epochs = 1000, # max number of epochs\n",
    "                 batch_size = 1000): # batch size for training\n",
    "        \n",
    "        assert len(hidden_dims) == n_hidden, \"Hidden dims mismatch!\"\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mode = mode\n",
    "        self.datapath = datapath\n",
    "        self.model_path = model_path\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # train, validation and test sets :\n",
    "        #self.tr, self.va, self.te = np.load(open(datapath, \"rb\"))\n",
    "        u = pickle._Unpickler(open(datapath, 'rb'))\n",
    "        u.encoding = 'latin1'\n",
    "        self.tr, self.va, self.te = u.load()\n",
    "\n",
    "    def initialize_weights(self, dims, method):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases according to the specified method\n",
    "        Parameters :\n",
    "        - dims: (list of two integers) - the size of input/output layers\n",
    "        - method: (string) - initializes the weight matrices\n",
    "            -> \"zero\" for a Zero initialisation of the weights\n",
    "            -> \"normal\" for a Normal initialisation of the weights\n",
    "            -> \"glorot\" for a Uniform initialisation of the weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            \n",
    "            self.weights = {}\n",
    "            all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
    "            print(\"Layers dimensions are : \", all_dims)\n",
    "            \n",
    "            for layer_n in range(1, self.n_hidden + 2):\n",
    "                if method == \"zero\":\n",
    "                    self.weights[f\"W{layer_n}\"] = np.zeros(shape=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                elif method == \"normal\":\n",
    "                    self.weights[f\"W{layer_n}\"] = np.random.normal(loc=0.0, scale=1.0, size=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                elif method == \"glorot\":\n",
    "                    b = np.sqrt(6.0/(all_dims[layer_n]+all_dims[layer_n-1]))\n",
    "                    self.weights[f\"W{layer_n}\"] = np.random.uniform(low=-1*b, high=b, size=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                else:\n",
    "                    raise Exception(\"The provided name for the initialization method is invalid.\")\n",
    "                print(\"Initialized W\",layer_n,\":\\n\",self.weights[f\"W{layer_n}\"])\n",
    "                self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))  # np.random.rand(1, all_dims[layer_n])\n",
    "                \n",
    "        elif self.mode == \"test\":\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Unknown Mode!\")\n",
    "\n",
    "    def activation(self, input, prime=False): # Prime for Heavyside, else ReLu\n",
    "        if prime:\n",
    "            return input > 0\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def loss(self, prediction, labels):  # Computes the cross entropy\n",
    "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
    "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
    "        return -1 * np.sum(labels * np.log(prediction) + (1-labels) * np.log(1-prediction)) / prediction.shape[0]\n",
    "\n",
    "    def softmax(self, input):  # Computes the softmax of the input\n",
    "        Z = np.exp(input - np.max(input)) # softmax(x+C) = softmax(x) (stability)\n",
    "        return Z / np.sum(Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, input):  # Forward propagation : computes the outputs (cache) from the input\n",
    "        # TODO : description\n",
    "        cache = {\"H0\": input}\n",
    "        for layer in range(1, self.n_hidden + 1):\n",
    "            cache[f\"A{layer}\"] = cache[f\"H{layer-1}\"] @ self.weights[f\"W{layer}\"] + self.weights[f\"b{layer}\"]\n",
    "            cache[f\"H{layer}\"] = self.activation(cache[f\"A{layer}\"])\n",
    "        layer = self.n_hidden + 1\n",
    "        cache[f\"A{layer}\"] = cache[f\"H{layer-1}\"] @ self.weights[f\"W{layer}\"] + self.weights[f\"b{layer}\"]\n",
    "        cache[f\"H{layer}\"] = self.softmax(cache[f\"A{layer}\"]) # softmax on last layer\n",
    "        return cache\n",
    "\n",
    "    def backward(self, cache, labels):  # Backward propagation : computes the gradients from the outputs (cache)\n",
    "        # TODO: description, gradient of the loss function?\n",
    "        output = cache[f\"H{self.n_hidden+1}\"]\n",
    "        grads = {\n",
    "            f\"dA{self.n_hidden+1}\": - (labels - output),\n",
    "            # f\"dA{self.n_hidden+1}\": - (labels - self.loss(output, labels))\n",
    "        }\n",
    "        for layer in range(self.n_hidden + 1, 0, -1):\n",
    "            # print(f\"Shape dA=\", grads[f\"dA{layer}\"].shape)\n",
    "            # print(f\"Shape H=\", cache[f\"H{layer-1}\"].shape)\n",
    "\n",
    "            grads[f\"dW{layer}\"] = cache[f\"H{layer-1}\"].T @ grads[f\"dA{layer}\"]\n",
    "            grads[f\"db{layer}\"] = grads[f\"dA{layer}\"]\n",
    "\n",
    "            if layer > 1:\n",
    "                grads[f\"dH{layer-1}\"] = grads[f\"dA{layer}\"] @ self.weights[f\"W{layer}\"].T\n",
    "                grads[f\"dA{layer-1}\"] = grads[f\"dH{layer-1}\"] * self.activation(cache[f\"A{layer-1}\"], prime=True)\n",
    "                # print(f\"Shape dA=\", grads[f\"dA{layer-1}\"].shape)\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads):  #\n",
    "        # print(grads.keys())\n",
    "        for layer in range(1, self.n_hidden + 1):\n",
    "            # print(grads[f\"dW{layer}\"].shape,self.weights[f\"W{layer}\"].shape)\n",
    "            self.weights[f\"W{layer}\"] = self.weights[f\"W{layer}\"] - self.lr * grads[f\"dW{layer}\"] / self.batch_size # why division ?\n",
    "            #self.weights[f\"b{layer}\"] = self.weights[f\"b{layer}\"] - self.lr * grads[f\"db{layer}\"] # / self.batch_size\n",
    "\n",
    "    def train(self, initializationMethod):\n",
    "        X_train, y_train = self.tr\n",
    "        y_onehot = np.eye(np.max(y_train) - np.min(y_train) + 1)[y_train]\n",
    "        # print(y_train.shape,y_onehot.shape)\n",
    "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
    "        self.initialize_weights(dims, initializationMethod)\n",
    "\n",
    "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            predictedY = np.zeros_like(y_train)\n",
    "            trainLoss = 0\n",
    "            for batch in range(n_batches):\n",
    "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                cache = self.forward(minibatchX)\n",
    "                grads = self.backward(cache, minibatchY)\n",
    "                self.update(grads)\n",
    "\n",
    "                trainLoss += self.loss(cache[f\"H{self.n_hidden+1}\"], minibatchY)\n",
    "                predictedY[self.batch_size * batch:self.batch_size * (batch + 1)] = np.argmax(\n",
    "                    cache[f\"H{self.n_hidden + 1}\"], axis=1)\n",
    "\n",
    "            X_val, y_val = self.va\n",
    "            onVal_y = np.eye(np.max(y_train) - np.min(y_train) + 1)[y_val]\n",
    "            valCache = self.forward(X_val)\n",
    "\n",
    "            predicted_valY = np.argmax(valCache[f\"H{self.n_hidden + 1}\"], axis=1)\n",
    "            valAccuracy = np.mean(y_val == predicted_valY)\n",
    "            valLoss = self.loss(valCache[f\"H{self.n_hidden+1}\"], onVal_y)\n",
    "\n",
    "            trAccuracy = np.mean(y_train == predictedY)\n",
    "\n",
    "            print(f\"Epoch= {epoch}, Loss={trainLoss:10.2f}, Accuracy={trAccuracy:3.6f}, Val.Loss={valLoss:10.2f}, Val.Accuracy= {valAccuracy:3.6f}\")\n",
    "            # break\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the NN class with MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neural_net = NN(datapath=\"mnist.pkl\", hidden_dims=(500, 400))\n",
    "neural_net.train(\"glorot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
