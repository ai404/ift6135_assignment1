{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 :\n",
    "\n",
    "#### Definition of the MLP class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# MLP class :\n",
    "\n",
    "# The model implemented as follows :\n",
    "# Each layers is represented by a b vector (biases) and a W matrix (weights)\n",
    "# These are referenced by the weights dictionary. The format is :\n",
    "# self.weights[f\"W{n}\"] to access the weights of the n-th layer\n",
    "# self.weights[f\"b{n}\"] to access the biases of the n-th layer\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_dims=(1024, 2048), # dimensions of each hidden layers\n",
    "                 n_hidden=2, # number of hidden layers\n",
    "                 mode='train', # current mode : train/test\n",
    "                 datapath=None, # path where to find the .pkl file\n",
    "                 model_path=None, # path where to save/load the model \n",
    "                 epsilon = 1e-6,\n",
    "                 lr = 1e-1, # learning rate\n",
    "                 n_epochs = 1000, # max number of epochs\n",
    "                 batch_size = 1000): # batch size for training\n",
    "        \n",
    "        assert len(hidden_dims) == n_hidden, \"Hidden dims mismatch!\"\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mode = mode\n",
    "        self.datapath = datapath\n",
    "        self.model_path = model_path\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # train, validation and test sets :\n",
    "        #self.tr, self.va, self.te = np.load(open(datapath, \"rb\"))\n",
    "        u = pickle._Unpickler(open(datapath, 'rb'))\n",
    "        u.encoding = 'latin1'\n",
    "        self.tr, self.va, self.te = u.load()\n",
    "\n",
    "    def initialize_weights(self, dims, method):\n",
    "        \"\"\"\n",
    "        Parameters :\n",
    "        - dims: (list of two integers) - the size of input/output layers\n",
    "        - method: (string) - initializes the weight matrices\n",
    "            -> \"zero\" for a Zero initialisation of the weights\n",
    "            -> \"normal\" for a Normal initialisation of the weights\n",
    "            -> \"glorot\" for a Uniform initialisation of the weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            \n",
    "            self.weights = {}\n",
    "            all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]]\n",
    "            print(\"Layers dimensions are : \", all_dims)\n",
    "            \n",
    "            for layer_n in range(1, self.n_hidden + 2):\n",
    "                if method == \"zero\":\n",
    "                    self.weights[f\"W{layer_n}\"] = np.zeros(shape=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                elif method == \"normal\":\n",
    "                    self.weights[f\"W{layer_n}\"] = np.random.normal(loc=0.0, scale=1.0, size=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                elif method == \"glorot\":\n",
    "                    b = np.sqrt(6.0/(all_dims[layer_n]+all_dims[layer_n-1]))\n",
    "                    self.weights[f\"W{layer_n}\"] = np.random.uniform(low=-1*b, high=b, size=(all_dims[layer_n - 1],all_dims[layer_n]))\n",
    "                else:\n",
    "                    raise Exception(\"The provided method name is invalid.\")\n",
    "                print(\"Initialized W\",layer_n,\":\\n\",self.weights[f\"W{layer_n}\"])\n",
    "                self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))  # np.random.rand(1, all_dims[layer_n])\n",
    "                \n",
    "        elif self.mode == \"test\":\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Unknown Mode!\")\n",
    "\n",
    "    def activation(self, input, prime=False): # Prime for Heavyside, else ReLu\n",
    "        if prime:\n",
    "            return input > 0\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def loss(self, prediction, labels):  #\n",
    "        # TODO\n",
    "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
    "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
    "        return - np.sum(labels * np.log(prediction)) # / prediction.shape[0]\n",
    "\n",
    "    def softmax(self, input):  # Computes the stable softmax of the input\n",
    "        Z = np.exp(input - np.max(input)) # softmax(x+C) = softmax(x)\n",
    "        return Z / np.sum(Z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, input):  #\n",
    "        cache = {\"H0\": input}\n",
    "        for layer in range(1, self.n_hidden + 1):\n",
    "            cache[f\"A{layer}\"] = cache[f\"H{layer-1}\"] @ self.weights[f\"W{layer}\"] + self.weights[f\"b{layer}\"]\n",
    "            cache[f\"H{layer}\"] = self.activation(cache[f\"A{layer}\"])\n",
    "\n",
    "        layer = self.n_hidden + 1\n",
    "        cache[f\"A{layer}\"] = cache[f\"H{layer-1}\"] @ self.weights[f\"W{layer}\"] + self.weights[f\"b{layer}\"]\n",
    "        cache[f\"H{layer}\"] = self.softmax(cache[f\"A{layer}\"]) # softmax on last layer\n",
    "        return cache\n",
    "\n",
    "    def backward(self, cache, labels):  #\n",
    "        # TODO\n",
    "        output = cache[f\"H{self.n_hidden+1}\"]\n",
    "        grads = {\n",
    "            f\"dA{self.n_hidden+1}\": - (labels - output),\n",
    "        }\n",
    "        for layer in range(self.n_hidden + 1, 0, -1):\n",
    "            # print(f\"Shape dA=\", grads[f\"dA{layer}\"].shape)\n",
    "            # print(f\"Shape H=\", cache[f\"H{layer-1}\"].shape)\n",
    "\n",
    "            grads[f\"dW{layer}\"] = cache[f\"H{layer-1}\"].T @ grads[f\"dA{layer}\"]\n",
    "            grads[f\"db{layer}\"] = grads[f\"dA{layer}\"]\n",
    "\n",
    "            if layer > 1:\n",
    "                grads[f\"dH{layer-1}\"] = grads[f\"dA{layer}\"] @ self.weights[f\"W{layer}\"].T\n",
    "                grads[f\"dA{layer-1}\"] = grads[f\"dH{layer-1}\"] * self.activation(cache[f\"A{layer-1}\"], prime=True)\n",
    "                # print(f\"Shape dA=\", grads[f\"dA{layer-1}\"].shape)\n",
    "        return grads\n",
    "\n",
    "    def update(self, grads):  #\n",
    "        # rint(grads.keys())\n",
    "        for layer in range(1, self.n_hidden + 1):\n",
    "            # print(grads[f\"dW{layer}\"].shape,self.weights[f\"W{layer}\"].shape)\n",
    "            self.weights[f\"W{layer}\"] = self.weights[f\"W{layer}\"] - self.lr * grads[f\"dW{layer}\"] / self.batch_size\n",
    "\n",
    "    def train(self, initializationMethod):\n",
    "        X_train, y_train = self.tr\n",
    "        y_onehot = np.eye(np.max(y_train) - np.min(y_train) + 1)[y_train]\n",
    "        # print(y_train.shape,y_onehot.shape)\n",
    "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
    "        self.initialize_weights(dims, initializationMethod)\n",
    "\n",
    "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            predictedY = np.zeros_like(y_train)\n",
    "            trainLoss = 0\n",
    "            for batch in range(n_batches):\n",
    "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
    "                cache = self.forward(minibatchX)\n",
    "                grads = self.backward(cache, minibatchY)\n",
    "                self.update(grads)\n",
    "\n",
    "                trainLoss += self.loss(cache[f\"H{self.n_hidden+1}\"], minibatchY)\n",
    "                predictedY[self.batch_size * batch:self.batch_size * (batch + 1)] = np.argmax(\n",
    "                    cache[f\"H{self.n_hidden + 1}\"], axis=1)\n",
    "\n",
    "            X_val, y_val = self.va\n",
    "            onVal_y = np.eye(np.max(y_train) - np.min(y_train) + 1)[y_val]\n",
    "            valCache = self.forward(X_val)\n",
    "\n",
    "            predicted_valY = np.argmax(valCache[f\"H{self.n_hidden + 1}\"], axis=1)\n",
    "            valAccuracy = np.mean(y_val == predicted_valY)\n",
    "            valLoss = self.loss(valCache[f\"H{self.n_hidden+1}\"], onVal_y)\n",
    "\n",
    "            trAccuracy = np.mean(y_train == predictedY)\n",
    "\n",
    "            print(f\"Epoch= {epoch}, Loss={trainLoss:10.2f}, Accuracy={trAccuracy:4.2f}, Val.Loss={valLoss:10.2f}, Val.Accuracy= {valAccuracy:4.2f}\")\n",
    "            # break\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the NN class with MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers dimensions are :  [784, 500, 400, 10]\n",
      "Initialized W 1 :\n",
      " [[ 0.00129058 -0.06596431  0.00814446 ... -0.05051122 -0.04019191\n",
      "   0.06466707]\n",
      " [ 0.04339839  0.0328401  -0.0558186  ... -0.02579991 -0.04871583\n",
      "  -0.00735755]\n",
      " [ 0.02227065 -0.04462931 -0.04940466 ... -0.03489078 -0.05014492\n",
      "   0.05607768]\n",
      " ...\n",
      " [-0.03234866 -0.023499    0.00110721 ...  0.02435768  0.06770049\n",
      "  -0.0466094 ]\n",
      " [ 0.03961624  0.0006465  -0.04126814 ...  0.01528632  0.01788926\n",
      "   0.03989433]\n",
      " [-0.03846133 -0.06437692  0.02346631 ... -0.06265702 -0.04148803\n",
      "   0.03378623]]\n",
      "Initialized W 2 :\n",
      " [[ 0.05752787  0.07532826  0.01655859 ... -0.03216579 -0.05613719\n",
      "  -0.03262965]\n",
      " [-0.04945394  0.04610535  0.01889513 ... -0.01169685  0.03838026\n",
      "  -0.05419353]\n",
      " [-0.00038338 -0.05000258  0.01893677 ... -0.04346026  0.03564136\n",
      "   0.03342679]\n",
      " ...\n",
      " [ 0.05693506 -0.07222667 -0.0349831  ... -0.06158319 -0.04302877\n",
      "  -0.03379674]\n",
      " [ 0.05441861 -0.07849314  0.01068567 ... -0.04923981  0.00451123\n",
      "   0.05469962]\n",
      " [ 0.00690586  0.04796973  0.01590694 ...  0.00728745 -0.0556953\n",
      "  -0.03100662]]\n",
      "Initialized W 3 :\n",
      " [[-0.04642791  0.036915    0.0228198  ...  0.08311355 -0.03727697\n",
      "   0.0241776 ]\n",
      " [ 0.07134042  0.07396665  0.09208994 ... -0.08203271  0.11402551\n",
      "  -0.02578505]\n",
      " [ 0.10163255  0.02128206 -0.01093943 ... -0.02910484  0.1073769\n",
      "   0.11344685]\n",
      " ...\n",
      " [-0.02526157 -0.06761973  0.03515961 ... -0.01920834  0.08346987\n",
      "   0.07513523]\n",
      " [ 0.06677677 -0.05284075 -0.10199949 ... -0.03597134  0.1163193\n",
      "   0.04356264]\n",
      " [ 0.03199239  0.03714849  0.02839093 ...  0.07065782 -0.09637496\n",
      "  -0.05344016]]\n",
      "Epoch= 0, Loss=  62638.16, Accuracy=0.72, Val.Loss=   6020.14, Val.Accuracy= 0.87\n",
      "Epoch= 1, Loss=  26334.67, Accuracy=0.87, Val.Loss=   4056.08, Val.Accuracy= 0.89\n",
      "Epoch= 2, Loss=  20535.71, Accuracy=0.89, Val.Loss=   3447.13, Val.Accuracy= 0.91\n",
      "Epoch= 3, Loss=  18064.23, Accuracy=0.90, Val.Loss=   3126.19, Val.Accuracy= 0.91\n",
      "Epoch= 4, Loss=  16562.36, Accuracy=0.91, Val.Loss=   2913.11, Val.Accuracy= 0.92\n",
      "Epoch= 5, Loss=  15489.48, Accuracy=0.91, Val.Loss=   2753.35, Val.Accuracy= 0.92\n",
      "Epoch= 6, Loss=  14652.46, Accuracy=0.92, Val.Loss=   2624.64, Val.Accuracy= 0.93\n",
      "Epoch= 7, Loss=  13961.20, Accuracy=0.92, Val.Loss=   2516.90, Val.Accuracy= 0.93\n",
      "Epoch= 8, Loss=  13370.65, Accuracy=0.93, Val.Loss=   2423.46, Val.Accuracy= 0.93\n",
      "Epoch= 9, Loss=  12852.73, Accuracy=0.93, Val.Loss=   2341.08, Val.Accuracy= 0.93\n",
      "Epoch= 10, Loss=  12391.35, Accuracy=0.93, Val.Loss=   2266.95, Val.Accuracy= 0.94\n",
      "Epoch= 11, Loss=  11974.43, Accuracy=0.93, Val.Loss=   2199.78, Val.Accuracy= 0.94\n",
      "Epoch= 12, Loss=  11593.51, Accuracy=0.94, Val.Loss=   2138.47, Val.Accuracy= 0.94\n",
      "Epoch= 13, Loss=  11241.08, Accuracy=0.94, Val.Loss=   2082.03, Val.Accuracy= 0.94\n",
      "Epoch= 14, Loss=  10913.28, Accuracy=0.94, Val.Loss=   2029.74, Val.Accuracy= 0.94\n",
      "Epoch= 15, Loss=  10607.30, Accuracy=0.94, Val.Loss=   1981.13, Val.Accuracy= 0.95\n",
      "Epoch= 16, Loss=  10321.37, Accuracy=0.94, Val.Loss=   1935.70, Val.Accuracy= 0.95\n",
      "Epoch= 17, Loss=  10052.05, Accuracy=0.94, Val.Loss=   1893.00, Val.Accuracy= 0.95\n",
      "Epoch= 18, Loss=   9797.88, Accuracy=0.95, Val.Loss=   1852.86, Val.Accuracy= 0.95\n",
      "Epoch= 19, Loss=   9556.99, Accuracy=0.95, Val.Loss=   1814.95, Val.Accuracy= 0.95\n",
      "Epoch= 20, Loss=   9328.39, Accuracy=0.95, Val.Loss=   1779.16, Val.Accuracy= 0.95\n",
      "Epoch= 21, Loss=   9111.21, Accuracy=0.95, Val.Loss=   1745.39, Val.Accuracy= 0.95\n",
      "Epoch= 22, Loss=   8904.56, Accuracy=0.95, Val.Loss=   1713.30, Val.Accuracy= 0.95\n",
      "Epoch= 23, Loss=   8707.38, Accuracy=0.95, Val.Loss=   1682.83, Val.Accuracy= 0.95\n",
      "Epoch= 24, Loss=   8519.10, Accuracy=0.95, Val.Loss=   1653.73, Val.Accuracy= 0.95\n",
      "Epoch= 25, Loss=   8338.53, Accuracy=0.95, Val.Loss=   1626.01, Val.Accuracy= 0.96\n",
      "Epoch= 26, Loss=   8165.72, Accuracy=0.95, Val.Loss=   1599.55, Val.Accuracy= 0.96\n",
      "Epoch= 27, Loss=   7999.77, Accuracy=0.95, Val.Loss=   1574.30, Val.Accuracy= 0.96\n",
      "Epoch= 28, Loss=   7840.39, Accuracy=0.96, Val.Loss=   1550.14, Val.Accuracy= 0.96\n",
      "Epoch= 29, Loss=   7687.16, Accuracy=0.96, Val.Loss=   1527.02, Val.Accuracy= 0.96\n",
      "Epoch= 30, Loss=   7539.38, Accuracy=0.96, Val.Loss=   1504.95, Val.Accuracy= 0.96\n",
      "Epoch= 31, Loss=   7397.15, Accuracy=0.96, Val.Loss=   1483.58, Val.Accuracy= 0.96\n",
      "Epoch= 32, Loss=   7260.38, Accuracy=0.96, Val.Loss=   1463.16, Val.Accuracy= 0.96\n",
      "Epoch= 33, Loss=   7128.47, Accuracy=0.96, Val.Loss=   1443.76, Val.Accuracy= 0.96\n",
      "Epoch= 34, Loss=   7001.06, Accuracy=0.96, Val.Loss=   1425.02, Val.Accuracy= 0.96\n",
      "Epoch= 35, Loss=   6877.75, Accuracy=0.96, Val.Loss=   1406.89, Val.Accuracy= 0.96\n",
      "Epoch= 36, Loss=   6758.50, Accuracy=0.96, Val.Loss=   1389.44, Val.Accuracy= 0.96\n",
      "Epoch= 37, Loss=   6643.02, Accuracy=0.96, Val.Loss=   1372.70, Val.Accuracy= 0.96\n",
      "Epoch= 38, Loss=   6531.18, Accuracy=0.96, Val.Loss=   1356.51, Val.Accuracy= 0.96\n",
      "Epoch= 39, Loss=   6422.77, Accuracy=0.96, Val.Loss=   1340.76, Val.Accuracy= 0.96\n",
      "Epoch= 40, Loss=   6317.53, Accuracy=0.96, Val.Loss=   1325.61, Val.Accuracy= 0.96\n",
      "Epoch= 41, Loss=   6215.08, Accuracy=0.97, Val.Loss=   1310.89, Val.Accuracy= 0.96\n",
      "Epoch= 42, Loss=   6115.50, Accuracy=0.97, Val.Loss=   1296.73, Val.Accuracy= 0.96\n",
      "Epoch= 43, Loss=   6018.93, Accuracy=0.97, Val.Loss=   1282.94, Val.Accuracy= 0.97\n",
      "Epoch= 44, Loss=   5925.07, Accuracy=0.97, Val.Loss=   1269.62, Val.Accuracy= 0.97\n",
      "Epoch= 45, Loss=   5833.82, Accuracy=0.97, Val.Loss=   1256.72, Val.Accuracy= 0.97\n",
      "Epoch= 46, Loss=   5745.04, Accuracy=0.97, Val.Loss=   1244.22, Val.Accuracy= 0.97\n",
      "Epoch= 47, Loss=   5658.85, Accuracy=0.97, Val.Loss=   1232.03, Val.Accuracy= 0.97\n",
      "Epoch= 48, Loss=   5574.93, Accuracy=0.97, Val.Loss=   1220.23, Val.Accuracy= 0.97\n",
      "Epoch= 49, Loss=   5493.14, Accuracy=0.97, Val.Loss=   1208.71, Val.Accuracy= 0.97\n",
      "Epoch= 50, Loss=   5413.59, Accuracy=0.97, Val.Loss=   1197.53, Val.Accuracy= 0.97\n",
      "Epoch= 51, Loss=   5336.09, Accuracy=0.97, Val.Loss=   1186.70, Val.Accuracy= 0.97\n",
      "Epoch= 52, Loss=   5260.51, Accuracy=0.97, Val.Loss=   1176.14, Val.Accuracy= 0.97\n",
      "Epoch= 53, Loss=   5186.74, Accuracy=0.97, Val.Loss=   1165.83, Val.Accuracy= 0.97\n",
      "Epoch= 54, Loss=   5114.68, Accuracy=0.97, Val.Loss=   1155.84, Val.Accuracy= 0.97\n",
      "Epoch= 55, Loss=   5044.34, Accuracy=0.97, Val.Loss=   1146.21, Val.Accuracy= 0.97\n",
      "Epoch= 56, Loss=   4975.74, Accuracy=0.97, Val.Loss=   1136.75, Val.Accuracy= 0.97\n",
      "Epoch= 57, Loss=   4908.90, Accuracy=0.97, Val.Loss=   1127.69, Val.Accuracy= 0.97\n",
      "Epoch= 58, Loss=   4843.64, Accuracy=0.97, Val.Loss=   1118.77, Val.Accuracy= 0.97\n",
      "Epoch= 59, Loss=   4779.88, Accuracy=0.97, Val.Loss=   1110.16, Val.Accuracy= 0.97\n",
      "Epoch= 60, Loss=   4717.45, Accuracy=0.97, Val.Loss=   1101.74, Val.Accuracy= 0.97\n",
      "Epoch= 61, Loss=   4656.52, Accuracy=0.98, Val.Loss=   1093.53, Val.Accuracy= 0.97\n",
      "Epoch= 62, Loss=   4596.86, Accuracy=0.98, Val.Loss=   1085.43, Val.Accuracy= 0.97\n",
      "Epoch= 63, Loss=   4538.57, Accuracy=0.98, Val.Loss=   1077.67, Val.Accuracy= 0.97\n",
      "Epoch= 64, Loss=   4481.39, Accuracy=0.98, Val.Loss=   1069.94, Val.Accuracy= 0.97\n",
      "Epoch= 65, Loss=   4425.44, Accuracy=0.98, Val.Loss=   1062.46, Val.Accuracy= 0.97\n",
      "Epoch= 66, Loss=   4370.58, Accuracy=0.98, Val.Loss=   1055.10, Val.Accuracy= 0.97\n",
      "Epoch= 67, Loss=   4316.88, Accuracy=0.98, Val.Loss=   1047.96, Val.Accuracy= 0.97\n",
      "Epoch= 68, Loss=   4264.28, Accuracy=0.98, Val.Loss=   1040.97, Val.Accuracy= 0.97\n",
      "Epoch= 69, Loss=   4212.63, Accuracy=0.98, Val.Loss=   1034.04, Val.Accuracy= 0.97\n",
      "Epoch= 70, Loss=   4162.05, Accuracy=0.98, Val.Loss=   1027.30, Val.Accuracy= 0.97\n",
      "Epoch= 71, Loss=   4112.54, Accuracy=0.98, Val.Loss=   1020.81, Val.Accuracy= 0.97\n",
      "Epoch= 72, Loss=   4063.93, Accuracy=0.98, Val.Loss=   1014.40, Val.Accuracy= 0.97\n",
      "Epoch= 73, Loss=   4016.25, Accuracy=0.98, Val.Loss=   1008.30, Val.Accuracy= 0.97\n",
      "Epoch= 74, Loss=   3969.42, Accuracy=0.98, Val.Loss=   1002.26, Val.Accuracy= 0.97\n",
      "Epoch= 75, Loss=   3923.43, Accuracy=0.98, Val.Loss=    996.28, Val.Accuracy= 0.97\n",
      "Epoch= 76, Loss=   3878.39, Accuracy=0.98, Val.Loss=    990.54, Val.Accuracy= 0.97\n",
      "Epoch= 77, Loss=   3834.16, Accuracy=0.98, Val.Loss=    984.93, Val.Accuracy= 0.97\n",
      "Epoch= 78, Loss=   3790.67, Accuracy=0.98, Val.Loss=    979.46, Val.Accuracy= 0.97\n",
      "Epoch= 79, Loss=   3748.08, Accuracy=0.98, Val.Loss=    974.10, Val.Accuracy= 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 80, Loss=   3706.12, Accuracy=0.98, Val.Loss=    968.97, Val.Accuracy= 0.97\n",
      "Epoch= 81, Loss=   3665.01, Accuracy=0.98, Val.Loss=    963.83, Val.Accuracy= 0.97\n",
      "Epoch= 82, Loss=   3624.61, Accuracy=0.98, Val.Loss=    958.85, Val.Accuracy= 0.97\n",
      "Epoch= 83, Loss=   3584.92, Accuracy=0.98, Val.Loss=    953.90, Val.Accuracy= 0.97\n",
      "Epoch= 84, Loss=   3545.99, Accuracy=0.98, Val.Loss=    949.15, Val.Accuracy= 0.97\n",
      "Epoch= 85, Loss=   3507.77, Accuracy=0.98, Val.Loss=    944.44, Val.Accuracy= 0.97\n",
      "Epoch= 86, Loss=   3470.19, Accuracy=0.98, Val.Loss=    939.79, Val.Accuracy= 0.97\n",
      "Epoch= 87, Loss=   3433.30, Accuracy=0.98, Val.Loss=    935.34, Val.Accuracy= 0.97\n",
      "Epoch= 88, Loss=   3397.09, Accuracy=0.98, Val.Loss=    930.95, Val.Accuracy= 0.97\n",
      "Epoch= 89, Loss=   3361.34, Accuracy=0.98, Val.Loss=    926.65, Val.Accuracy= 0.97\n",
      "Epoch= 90, Loss=   3326.22, Accuracy=0.98, Val.Loss=    922.43, Val.Accuracy= 0.97\n",
      "Epoch= 91, Loss=   3291.69, Accuracy=0.98, Val.Loss=    918.31, Val.Accuracy= 0.97\n",
      "Epoch= 92, Loss=   3257.78, Accuracy=0.98, Val.Loss=    914.23, Val.Accuracy= 0.97\n",
      "Epoch= 93, Loss=   3224.32, Accuracy=0.98, Val.Loss=    910.21, Val.Accuracy= 0.97\n",
      "Epoch= 94, Loss=   3191.37, Accuracy=0.98, Val.Loss=    906.35, Val.Accuracy= 0.97\n",
      "Epoch= 95, Loss=   3159.02, Accuracy=0.98, Val.Loss=    902.56, Val.Accuracy= 0.97\n",
      "Epoch= 96, Loss=   3127.15, Accuracy=0.98, Val.Loss=    898.86, Val.Accuracy= 0.98\n",
      "Epoch= 97, Loss=   3095.79, Accuracy=0.98, Val.Loss=    895.11, Val.Accuracy= 0.98\n",
      "Epoch= 98, Loss=   3065.01, Accuracy=0.98, Val.Loss=    891.52, Val.Accuracy= 0.98\n",
      "Epoch= 99, Loss=   3034.57, Accuracy=0.98, Val.Loss=    887.98, Val.Accuracy= 0.98\n",
      "Epoch= 100, Loss=   3004.72, Accuracy=0.99, Val.Loss=    884.51, Val.Accuracy= 0.98\n",
      "Epoch= 101, Loss=   2975.26, Accuracy=0.99, Val.Loss=    881.08, Val.Accuracy= 0.98\n",
      "Epoch= 102, Loss=   2946.30, Accuracy=0.99, Val.Loss=    877.71, Val.Accuracy= 0.98\n",
      "Epoch= 103, Loss=   2917.72, Accuracy=0.99, Val.Loss=    874.49, Val.Accuracy= 0.98\n",
      "Epoch= 104, Loss=   2889.80, Accuracy=0.99, Val.Loss=    871.26, Val.Accuracy= 0.98\n",
      "Epoch= 105, Loss=   2861.99, Accuracy=0.99, Val.Loss=    868.15, Val.Accuracy= 0.98\n",
      "Epoch= 106, Loss=   2834.76, Accuracy=0.99, Val.Loss=    865.00, Val.Accuracy= 0.98\n",
      "Epoch= 107, Loss=   2807.93, Accuracy=0.99, Val.Loss=    861.99, Val.Accuracy= 0.98\n",
      "Epoch= 108, Loss=   2781.47, Accuracy=0.99, Val.Loss=    859.01, Val.Accuracy= 0.98\n",
      "Epoch= 109, Loss=   2755.48, Accuracy=0.99, Val.Loss=    856.10, Val.Accuracy= 0.98\n",
      "Epoch= 110, Loss=   2729.76, Accuracy=0.99, Val.Loss=    853.25, Val.Accuracy= 0.98\n",
      "Epoch= 111, Loss=   2704.53, Accuracy=0.99, Val.Loss=    850.44, Val.Accuracy= 0.98\n",
      "Epoch= 112, Loss=   2679.56, Accuracy=0.99, Val.Loss=    847.76, Val.Accuracy= 0.98\n",
      "Epoch= 113, Loss=   2654.93, Accuracy=0.99, Val.Loss=    845.07, Val.Accuracy= 0.98\n",
      "Epoch= 114, Loss=   2630.58, Accuracy=0.99, Val.Loss=    842.42, Val.Accuracy= 0.98\n",
      "Epoch= 115, Loss=   2606.67, Accuracy=0.99, Val.Loss=    839.82, Val.Accuracy= 0.98\n",
      "Epoch= 116, Loss=   2583.05, Accuracy=0.99, Val.Loss=    837.37, Val.Accuracy= 0.98\n",
      "Epoch= 117, Loss=   2559.74, Accuracy=0.99, Val.Loss=    834.70, Val.Accuracy= 0.98\n",
      "Epoch= 118, Loss=   2536.72, Accuracy=0.99, Val.Loss=    832.35, Val.Accuracy= 0.98\n",
      "Epoch= 119, Loss=   2513.96, Accuracy=0.99, Val.Loss=    829.88, Val.Accuracy= 0.98\n",
      "Epoch= 120, Loss=   2491.62, Accuracy=0.99, Val.Loss=    827.53, Val.Accuracy= 0.98\n",
      "Epoch= 121, Loss=   2469.54, Accuracy=0.99, Val.Loss=    825.24, Val.Accuracy= 0.98\n",
      "Epoch= 122, Loss=   2447.75, Accuracy=0.99, Val.Loss=    822.93, Val.Accuracy= 0.98\n",
      "Epoch= 123, Loss=   2426.22, Accuracy=0.99, Val.Loss=    820.75, Val.Accuracy= 0.98\n",
      "Epoch= 124, Loss=   2405.02, Accuracy=0.99, Val.Loss=    818.45, Val.Accuracy= 0.98\n",
      "Epoch= 125, Loss=   2384.03, Accuracy=0.99, Val.Loss=    816.26, Val.Accuracy= 0.98\n",
      "Epoch= 126, Loss=   2363.28, Accuracy=0.99, Val.Loss=    814.10, Val.Accuracy= 0.98\n",
      "Epoch= 127, Loss=   2342.88, Accuracy=0.99, Val.Loss=    812.01, Val.Accuracy= 0.98\n",
      "Epoch= 128, Loss=   2322.69, Accuracy=0.99, Val.Loss=    809.96, Val.Accuracy= 0.98\n",
      "Epoch= 129, Loss=   2302.75, Accuracy=0.99, Val.Loss=    807.78, Val.Accuracy= 0.98\n",
      "Epoch= 130, Loss=   2283.06, Accuracy=0.99, Val.Loss=    805.76, Val.Accuracy= 0.98\n",
      "Epoch= 131, Loss=   2263.62, Accuracy=0.99, Val.Loss=    803.75, Val.Accuracy= 0.98\n",
      "Epoch= 132, Loss=   2244.37, Accuracy=0.99, Val.Loss=    801.84, Val.Accuracy= 0.98\n",
      "Epoch= 133, Loss=   2225.45, Accuracy=0.99, Val.Loss=    799.92, Val.Accuracy= 0.98\n",
      "Epoch= 134, Loss=   2206.67, Accuracy=0.99, Val.Loss=    798.04, Val.Accuracy= 0.98\n",
      "Epoch= 135, Loss=   2188.20, Accuracy=0.99, Val.Loss=    796.13, Val.Accuracy= 0.98\n",
      "Epoch= 136, Loss=   2169.93, Accuracy=0.99, Val.Loss=    794.28, Val.Accuracy= 0.98\n",
      "Epoch= 137, Loss=   2151.88, Accuracy=0.99, Val.Loss=    792.52, Val.Accuracy= 0.98\n",
      "Epoch= 138, Loss=   2134.05, Accuracy=0.99, Val.Loss=    790.76, Val.Accuracy= 0.98\n",
      "Epoch= 139, Loss=   2116.40, Accuracy=0.99, Val.Loss=    789.00, Val.Accuracy= 0.98\n",
      "Epoch= 140, Loss=   2098.93, Accuracy=0.99, Val.Loss=    787.23, Val.Accuracy= 0.98\n",
      "Epoch= 141, Loss=   2081.73, Accuracy=0.99, Val.Loss=    785.60, Val.Accuracy= 0.98\n",
      "Epoch= 142, Loss=   2064.66, Accuracy=0.99, Val.Loss=    783.93, Val.Accuracy= 0.98\n",
      "Epoch= 143, Loss=   2047.90, Accuracy=0.99, Val.Loss=    782.26, Val.Accuracy= 0.98\n",
      "Epoch= 144, Loss=   2031.23, Accuracy=0.99, Val.Loss=    780.62, Val.Accuracy= 0.98\n",
      "Epoch= 145, Loss=   2014.76, Accuracy=0.99, Val.Loss=    779.07, Val.Accuracy= 0.98\n",
      "Epoch= 146, Loss=   1998.57, Accuracy=0.99, Val.Loss=    777.47, Val.Accuracy= 0.98\n",
      "Epoch= 147, Loss=   1982.55, Accuracy=0.99, Val.Loss=    775.97, Val.Accuracy= 0.98\n",
      "Epoch= 148, Loss=   1966.67, Accuracy=0.99, Val.Loss=    774.40, Val.Accuracy= 0.98\n",
      "Epoch= 149, Loss=   1950.99, Accuracy=0.99, Val.Loss=    772.97, Val.Accuracy= 0.98\n",
      "Epoch= 150, Loss=   1935.49, Accuracy=0.99, Val.Loss=    771.40, Val.Accuracy= 0.98\n",
      "Epoch= 151, Loss=   1920.11, Accuracy=0.99, Val.Loss=    770.00, Val.Accuracy= 0.98\n",
      "Epoch= 152, Loss=   1905.03, Accuracy=0.99, Val.Loss=    768.56, Val.Accuracy= 0.98\n",
      "Epoch= 153, Loss=   1889.91, Accuracy=0.99, Val.Loss=    767.15, Val.Accuracy= 0.98\n",
      "Epoch= 154, Loss=   1875.08, Accuracy=0.99, Val.Loss=    765.74, Val.Accuracy= 0.98\n",
      "Epoch= 155, Loss=   1860.41, Accuracy=0.99, Val.Loss=    764.43, Val.Accuracy= 0.98\n",
      "Epoch= 156, Loss=   1845.88, Accuracy=0.99, Val.Loss=    763.12, Val.Accuracy= 0.98\n",
      "Epoch= 157, Loss=   1831.44, Accuracy=0.99, Val.Loss=    761.75, Val.Accuracy= 0.98\n",
      "Epoch= 158, Loss=   1817.23, Accuracy=0.99, Val.Loss=    760.47, Val.Accuracy= 0.98\n",
      "Epoch= 159, Loss=   1803.23, Accuracy=0.99, Val.Loss=    759.16, Val.Accuracy= 0.98\n",
      "Epoch= 160, Loss=   1789.19, Accuracy=0.99, Val.Loss=    757.94, Val.Accuracy= 0.98\n",
      "Epoch= 161, Loss=   1775.39, Accuracy=0.99, Val.Loss=    756.68, Val.Accuracy= 0.98\n",
      "Epoch= 162, Loss=   1761.78, Accuracy=0.99, Val.Loss=    755.42, Val.Accuracy= 0.98\n",
      "Epoch= 163, Loss=   1748.27, Accuracy=0.99, Val.Loss=    754.26, Val.Accuracy= 0.98\n",
      "Epoch= 164, Loss=   1734.87, Accuracy=0.99, Val.Loss=    753.14, Val.Accuracy= 0.98\n",
      "Epoch= 165, Loss=   1721.68, Accuracy=0.99, Val.Loss=    751.95, Val.Accuracy= 0.98\n",
      "Epoch= 166, Loss=   1708.60, Accuracy=0.99, Val.Loss=    750.81, Val.Accuracy= 0.98\n",
      "Epoch= 167, Loss=   1695.67, Accuracy=0.99, Val.Loss=    749.77, Val.Accuracy= 0.98\n",
      "Epoch= 168, Loss=   1682.88, Accuracy=0.99, Val.Loss=    748.63, Val.Accuracy= 0.98\n",
      "Epoch= 169, Loss=   1670.22, Accuracy=0.99, Val.Loss=    747.60, Val.Accuracy= 0.98\n",
      "Epoch= 170, Loss=   1657.72, Accuracy=0.99, Val.Loss=    746.60, Val.Accuracy= 0.98\n",
      "Epoch= 171, Loss=   1645.41, Accuracy=0.99, Val.Loss=    745.57, Val.Accuracy= 0.98\n",
      "Epoch= 172, Loss=   1633.06, Accuracy=0.99, Val.Loss=    744.50, Val.Accuracy= 0.98\n",
      "Epoch= 173, Loss=   1620.91, Accuracy=0.99, Val.Loss=    743.54, Val.Accuracy= 0.98\n",
      "Epoch= 174, Loss=   1608.91, Accuracy=0.99, Val.Loss=    742.57, Val.Accuracy= 0.98\n",
      "Epoch= 175, Loss=   1596.99, Accuracy=0.99, Val.Loss=    741.61, Val.Accuracy= 0.98\n",
      "Epoch= 176, Loss=   1585.23, Accuracy=0.99, Val.Loss=    740.65, Val.Accuracy= 0.98\n",
      "Epoch= 177, Loss=   1573.54, Accuracy=0.99, Val.Loss=    739.66, Val.Accuracy= 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 178, Loss=   1561.97, Accuracy=0.99, Val.Loss=    738.81, Val.Accuracy= 0.98\n",
      "Epoch= 179, Loss=   1550.61, Accuracy=0.99, Val.Loss=    737.86, Val.Accuracy= 0.98\n",
      "Epoch= 180, Loss=   1539.26, Accuracy=0.99, Val.Loss=    736.93, Val.Accuracy= 0.98\n",
      "Epoch= 181, Loss=   1528.02, Accuracy=0.99, Val.Loss=    736.10, Val.Accuracy= 0.98\n",
      "Epoch= 182, Loss=   1516.99, Accuracy=0.99, Val.Loss=    735.15, Val.Accuracy= 0.98\n",
      "Epoch= 183, Loss=   1506.00, Accuracy=0.99, Val.Loss=    734.33, Val.Accuracy= 0.98\n",
      "Epoch= 184, Loss=   1495.13, Accuracy=0.99, Val.Loss=    733.47, Val.Accuracy= 0.98\n",
      "Epoch= 185, Loss=   1484.44, Accuracy=0.99, Val.Loss=    732.56, Val.Accuracy= 0.98\n",
      "Epoch= 186, Loss=   1473.77, Accuracy=0.99, Val.Loss=    731.75, Val.Accuracy= 0.98\n",
      "Epoch= 187, Loss=   1463.31, Accuracy=0.99, Val.Loss=    730.93, Val.Accuracy= 0.98\n",
      "Epoch= 188, Loss=   1452.82, Accuracy=0.99, Val.Loss=    730.10, Val.Accuracy= 0.98\n",
      "Epoch= 189, Loss=   1442.52, Accuracy=0.99, Val.Loss=    729.37, Val.Accuracy= 0.98\n",
      "Epoch= 190, Loss=   1432.30, Accuracy=0.99, Val.Loss=    728.52, Val.Accuracy= 0.98\n",
      "Epoch= 191, Loss=   1422.18, Accuracy=0.99, Val.Loss=    727.72, Val.Accuracy= 0.98\n",
      "Epoch= 192, Loss=   1412.18, Accuracy=0.99, Val.Loss=    726.98, Val.Accuracy= 0.98\n",
      "Epoch= 193, Loss=   1402.33, Accuracy=0.99, Val.Loss=    726.24, Val.Accuracy= 0.98\n",
      "Epoch= 194, Loss=   1392.49, Accuracy=0.99, Val.Loss=    725.42, Val.Accuracy= 0.98\n",
      "Epoch= 195, Loss=   1382.76, Accuracy=0.99, Val.Loss=    724.76, Val.Accuracy= 0.98\n",
      "Epoch= 196, Loss=   1373.11, Accuracy=0.99, Val.Loss=    724.04, Val.Accuracy= 0.98\n",
      "Epoch= 197, Loss=   1363.64, Accuracy=0.99, Val.Loss=    723.36, Val.Accuracy= 0.98\n",
      "Epoch= 198, Loss=   1354.14, Accuracy=0.99, Val.Loss=    722.69, Val.Accuracy= 0.98\n",
      "Epoch= 199, Loss=   1344.77, Accuracy=0.99, Val.Loss=    722.00, Val.Accuracy= 0.98\n",
      "Epoch= 200, Loss=   1335.51, Accuracy=1.00, Val.Loss=    721.35, Val.Accuracy= 0.98\n",
      "Epoch= 201, Loss=   1326.32, Accuracy=1.00, Val.Loss=    720.69, Val.Accuracy= 0.98\n",
      "Epoch= 202, Loss=   1317.25, Accuracy=1.00, Val.Loss=    720.07, Val.Accuracy= 0.98\n",
      "Epoch= 203, Loss=   1308.20, Accuracy=1.00, Val.Loss=    719.42, Val.Accuracy= 0.98\n",
      "Epoch= 204, Loss=   1299.33, Accuracy=1.00, Val.Loss=    718.83, Val.Accuracy= 0.98\n",
      "Epoch= 205, Loss=   1290.48, Accuracy=1.00, Val.Loss=    718.10, Val.Accuracy= 0.98\n",
      "Epoch= 206, Loss=   1281.67, Accuracy=1.00, Val.Loss=    717.57, Val.Accuracy= 0.98\n",
      "Epoch= 207, Loss=   1272.99, Accuracy=1.00, Val.Loss=    716.95, Val.Accuracy= 0.98\n",
      "Epoch= 208, Loss=   1264.43, Accuracy=1.00, Val.Loss=    716.35, Val.Accuracy= 0.98\n",
      "Epoch= 209, Loss=   1255.89, Accuracy=1.00, Val.Loss=    715.75, Val.Accuracy= 0.98\n",
      "Epoch= 210, Loss=   1247.46, Accuracy=1.00, Val.Loss=    715.15, Val.Accuracy= 0.98\n",
      "Epoch= 211, Loss=   1239.09, Accuracy=1.00, Val.Loss=    714.62, Val.Accuracy= 0.98\n",
      "Epoch= 212, Loss=   1230.82, Accuracy=1.00, Val.Loss=    714.07, Val.Accuracy= 0.98\n",
      "Epoch= 213, Loss=   1222.64, Accuracy=1.00, Val.Loss=    713.50, Val.Accuracy= 0.98\n",
      "Epoch= 214, Loss=   1214.49, Accuracy=1.00, Val.Loss=    712.91, Val.Accuracy= 0.98\n",
      "Epoch= 215, Loss=   1206.40, Accuracy=1.00, Val.Loss=    712.43, Val.Accuracy= 0.98\n",
      "Epoch= 216, Loss=   1198.44, Accuracy=1.00, Val.Loss=    711.92, Val.Accuracy= 0.98\n",
      "Epoch= 217, Loss=   1190.51, Accuracy=1.00, Val.Loss=    711.36, Val.Accuracy= 0.98\n",
      "Epoch= 218, Loss=   1182.65, Accuracy=1.00, Val.Loss=    710.92, Val.Accuracy= 0.98\n",
      "Epoch= 219, Loss=   1174.89, Accuracy=1.00, Val.Loss=    710.37, Val.Accuracy= 0.98\n",
      "Epoch= 220, Loss=   1167.18, Accuracy=1.00, Val.Loss=    709.85, Val.Accuracy= 0.98\n",
      "Epoch= 221, Loss=   1159.54, Accuracy=1.00, Val.Loss=    709.40, Val.Accuracy= 0.98\n",
      "Epoch= 222, Loss=   1152.02, Accuracy=1.00, Val.Loss=    708.92, Val.Accuracy= 0.98\n",
      "Epoch= 223, Loss=   1144.51, Accuracy=1.00, Val.Loss=    708.46, Val.Accuracy= 0.98\n",
      "Epoch= 224, Loss=   1137.13, Accuracy=1.00, Val.Loss=    708.00, Val.Accuracy= 0.98\n",
      "Epoch= 225, Loss=   1129.78, Accuracy=1.00, Val.Loss=    707.59, Val.Accuracy= 0.98\n",
      "Epoch= 226, Loss=   1122.50, Accuracy=1.00, Val.Loss=    707.16, Val.Accuracy= 0.98\n",
      "Epoch= 227, Loss=   1115.27, Accuracy=1.00, Val.Loss=    706.67, Val.Accuracy= 0.98\n",
      "Epoch= 228, Loss=   1108.13, Accuracy=1.00, Val.Loss=    706.21, Val.Accuracy= 0.98\n",
      "Epoch= 229, Loss=   1101.05, Accuracy=1.00, Val.Loss=    705.82, Val.Accuracy= 0.98\n",
      "Epoch= 230, Loss=   1094.08, Accuracy=1.00, Val.Loss=    705.36, Val.Accuracy= 0.98\n",
      "Epoch= 231, Loss=   1087.09, Accuracy=1.00, Val.Loss=    704.98, Val.Accuracy= 0.98\n",
      "Epoch= 232, Loss=   1080.18, Accuracy=1.00, Val.Loss=    704.50, Val.Accuracy= 0.98\n",
      "Epoch= 233, Loss=   1073.38, Accuracy=1.00, Val.Loss=    704.17, Val.Accuracy= 0.98\n",
      "Epoch= 234, Loss=   1066.63, Accuracy=1.00, Val.Loss=    703.78, Val.Accuracy= 0.98\n",
      "Epoch= 235, Loss=   1059.87, Accuracy=1.00, Val.Loss=    703.36, Val.Accuracy= 0.98\n",
      "Epoch= 236, Loss=   1053.21, Accuracy=1.00, Val.Loss=    702.91, Val.Accuracy= 0.98\n",
      "Epoch= 237, Loss=   1046.64, Accuracy=1.00, Val.Loss=    702.54, Val.Accuracy= 0.98\n",
      "Epoch= 238, Loss=   1040.06, Accuracy=1.00, Val.Loss=    702.19, Val.Accuracy= 0.98\n",
      "Epoch= 239, Loss=   1033.61, Accuracy=1.00, Val.Loss=    701.72, Val.Accuracy= 0.98\n",
      "Epoch= 240, Loss=   1027.19, Accuracy=1.00, Val.Loss=    701.42, Val.Accuracy= 0.98\n",
      "Epoch= 241, Loss=   1020.83, Accuracy=1.00, Val.Loss=    701.02, Val.Accuracy= 0.98\n",
      "Epoch= 242, Loss=   1014.46, Accuracy=1.00, Val.Loss=    700.61, Val.Accuracy= 0.98\n",
      "Epoch= 243, Loss=   1008.20, Accuracy=1.00, Val.Loss=    700.31, Val.Accuracy= 0.98\n",
      "Epoch= 244, Loss=   1002.04, Accuracy=1.00, Val.Loss=    699.97, Val.Accuracy= 0.98\n",
      "Epoch= 245, Loss=    995.86, Accuracy=1.00, Val.Loss=    699.53, Val.Accuracy= 0.98\n",
      "Epoch= 246, Loss=    989.76, Accuracy=1.00, Val.Loss=    699.22, Val.Accuracy= 0.98\n",
      "Epoch= 247, Loss=    983.68, Accuracy=1.00, Val.Loss=    698.87, Val.Accuracy= 0.98\n",
      "Epoch= 248, Loss=    977.70, Accuracy=1.00, Val.Loss=    698.53, Val.Accuracy= 0.98\n",
      "Epoch= 249, Loss=    971.70, Accuracy=1.00, Val.Loss=    698.23, Val.Accuracy= 0.98\n",
      "Epoch= 250, Loss=    965.82, Accuracy=1.00, Val.Loss=    697.90, Val.Accuracy= 0.98\n",
      "Epoch= 251, Loss=    959.98, Accuracy=1.00, Val.Loss=    697.55, Val.Accuracy= 0.98\n",
      "Epoch= 252, Loss=    954.14, Accuracy=1.00, Val.Loss=    697.29, Val.Accuracy= 0.98\n",
      "Epoch= 253, Loss=    948.40, Accuracy=1.00, Val.Loss=    696.97, Val.Accuracy= 0.98\n",
      "Epoch= 254, Loss=    942.68, Accuracy=1.00, Val.Loss=    696.71, Val.Accuracy= 0.98\n",
      "Epoch= 255, Loss=    937.01, Accuracy=1.00, Val.Loss=    696.39, Val.Accuracy= 0.98\n",
      "Epoch= 256, Loss=    931.42, Accuracy=1.00, Val.Loss=    696.09, Val.Accuracy= 0.98\n",
      "Epoch= 257, Loss=    925.84, Accuracy=1.00, Val.Loss=    695.85, Val.Accuracy= 0.98\n",
      "Epoch= 258, Loss=    920.32, Accuracy=1.00, Val.Loss=    695.52, Val.Accuracy= 0.98\n",
      "Epoch= 259, Loss=    914.85, Accuracy=1.00, Val.Loss=    695.25, Val.Accuracy= 0.98\n",
      "Epoch= 260, Loss=    909.43, Accuracy=1.00, Val.Loss=    695.00, Val.Accuracy= 0.98\n",
      "Epoch= 261, Loss=    904.00, Accuracy=1.00, Val.Loss=    694.66, Val.Accuracy= 0.98\n",
      "Epoch= 262, Loss=    898.68, Accuracy=1.00, Val.Loss=    694.48, Val.Accuracy= 0.98\n",
      "Epoch= 263, Loss=    893.38, Accuracy=1.00, Val.Loss=    694.14, Val.Accuracy= 0.98\n",
      "Epoch= 264, Loss=    888.12, Accuracy=1.00, Val.Loss=    693.96, Val.Accuracy= 0.98\n",
      "Epoch= 265, Loss=    882.93, Accuracy=1.00, Val.Loss=    693.65, Val.Accuracy= 0.98\n",
      "Epoch= 266, Loss=    877.77, Accuracy=1.00, Val.Loss=    693.43, Val.Accuracy= 0.98\n",
      "Epoch= 267, Loss=    872.65, Accuracy=1.00, Val.Loss=    693.20, Val.Accuracy= 0.98\n",
      "Epoch= 268, Loss=    867.57, Accuracy=1.00, Val.Loss=    692.97, Val.Accuracy= 0.98\n",
      "Epoch= 269, Loss=    862.52, Accuracy=1.00, Val.Loss=    692.69, Val.Accuracy= 0.98\n",
      "Epoch= 270, Loss=    857.53, Accuracy=1.00, Val.Loss=    692.55, Val.Accuracy= 0.98\n",
      "Epoch= 271, Loss=    852.53, Accuracy=1.00, Val.Loss=    692.32, Val.Accuracy= 0.98\n",
      "Epoch= 272, Loss=    847.63, Accuracy=1.00, Val.Loss=    692.05, Val.Accuracy= 0.98\n",
      "Epoch= 273, Loss=    842.74, Accuracy=1.00, Val.Loss=    691.83, Val.Accuracy= 0.98\n",
      "Epoch= 274, Loss=    837.90, Accuracy=1.00, Val.Loss=    691.61, Val.Accuracy= 0.98\n",
      "Epoch= 275, Loss=    833.10, Accuracy=1.00, Val.Loss=    691.34, Val.Accuracy= 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 276, Loss=    828.39, Accuracy=1.00, Val.Loss=    691.23, Val.Accuracy= 0.98\n",
      "Epoch= 277, Loss=    823.62, Accuracy=1.00, Val.Loss=    690.96, Val.Accuracy= 0.98\n"
     ]
    }
   ],
   "source": [
    "neural_net = NN(datapath=\"mnist.pkl\", hidden_dims=(500, 400))\n",
    "neural_net.train(\"glorot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
